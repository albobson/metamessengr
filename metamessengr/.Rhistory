NA, #true
nchar(content)), #FALSE ## Messenger only allows messages of 640 characters or less. Setting anything
## above that to NA as anything else is bogus
convo = paste(sender_name,"-",sent_to)) %>%
rename(sender = sender_name)
tidy_text <- dat %>%
mutate(content_c = str_replace_all(content, "[^a-zA-Z0-9]", " "), #remove special characters
content_c = str_to_lower(content_c),                       #make lowercase
content_c = removeWords(content_c, stop_words$word),        #remove stop words
content_c = str_squish(content_c),                        #remove whitespace
) %>%
filter(!is.na(content_c),
content_c != "",
content_c != "connected messenger")
View(tidy_text)
tidy_text <- dat %>%
mutate(content_c = str_replace_all(content, "[^a-zA-Z0-9]", " "), #remove special characters
content_c = str_to_lower(content_c),                       #make lowercase
content_c = removeWords(content_c, stop_words$word),        #remove stop words
content_c = str_squish(content_c),                        #remove whitespace
) %>%
filter(!is.na(content_c),
content_c != "",
content_c == "connected messenger")
View(tidy_text)
tidy_text <- dat %>%
mutate(content_c = str_replace_all(content, "[^a-zA-Z0-9]", " "), #remove special characters
content_c = str_to_lower(content_c),                       #make lowercase
content_c = removeWords(content_c, stop_words$word),        #remove stop words
content_c = str_squish(content_c),                        #remove whitespace
) %>%
filter(!is.na(content_c),
content_c != "",
content_c != "connected messenger")
#TOBY IS GETTING FORKED!
### Script to analyze messenger data ###
using<-function(...) {
libs<-unlist(list(...))
req<-unlist(lapply(libs,require,character.only=TRUE))
need<-libs[req==FALSE]
if(length(need)>0){
install.packages(need)
lapply(need,require,character.only=TRUE)
}
}
using("purrr","jsonlite","dplyr","tidytext","tidyr","textdata","stringr",
"ggplot2","scales","chron","lexicon","ggraph","igraph", "tm")
## Set wd
setwd("messages/inbox")
#extract name of messenger
## Writing a function for just selection
selection <- function(inp=NULL) {
#creates list of file names
fns = list.files(recursive = T,
pattern = "\\.json$")
#extracts groups from file name
fns2 = str_extract(fns, "[^_]+")
## This looks for all of the files that start with "message_" and are .json
## format. It then reads each file in using fromJSON() and assigns to inp
inp = lapply(fns,
fromJSON)
## This then takes the inp and finds how many files it read and assigns it l
l = as.numeric(length(map(inp,2)))
## s is called as a sequence from 1 to the previous length (l)
s = seq(from = 1, to = l)
## Setting f to NULL
f = NULL
## For i in the number of files loaded, select the timestamp, sender_name, and
## content of the message. It then makes f = to that selection if it's the
## first file's data, or it row binds the data onto it if it's anything else.
for (i in s) {
if("content" %in% names(inp[[i]][[2]])){
t = map(inp,2)[[i]] %>%
select(timestamp_ms, sender_name, content) %>%
mutate(sent_to = fns2[[i]])
if (i == 1){
f=t
} else {
f = rbind(f, t)
}
}
print(i)
}
## Then f, the smaller dataset, is bound to inp and is the output
inp = f
}
n = selection()
#*******************************************************
#Data Cleaning ####
#*******************************************************
#TIME CLEANING
dat = n %>%
mutate(timestamp_ms = timestamp_ms/1000,## Turn the timestamp into the format that R likes (not miliseconds)
timestamp_ms = as.POSIXct(timestamp_ms,
origin="1970-01-01",
tz="America/Los_Angeles"), ## Have it convert the timestamp to POSIXct
date = as.Date(timestamp_ms),
time = strftime(timestamp_ms, format="%H:%M:%S"),
time = chron(times=time), ## Further time cleaning
time = as.numeric(time),
hour = time *24, ## New column with sending hour
length = ifelse(nchar(content)>640, #nchar >640
NA, #true
nchar(content)), #FALSE ## Messenger only allows messages of 640 characters or less. Setting anything
## above that to NA as anything else is bogus
convo = paste(sender_name,"-",sent_to)) %>%
rename(sender = sender_name)
#CLEANING THE CONTENT
tidy_text <- dat %>%
mutate(content_c = str_replace_all(content, "[^a-zA-Z0-9]", " "), #remove special characters
content_c = str_to_lower(content_c),                       #make lowercase
content_c = removeWords(content_c, stop_words$word),        #remove stop words
content_c = str_squish(content_c),                        #remove whitespace
) %>%
filter(!is.na(content_c),
content_c != "",
content_c != "connected messenger") #removes default message from facebook when you become fb friends with someone
tidy_words = unnest_tokens(tidy_text,
input = content_c,
output = "words",
token = "words")
#SUMMARY ####
dat_s = tidy_text %>%
group_by(sender,sent_to) %>%
summarise(count = n(),
length = sum(length, na.rm = T))
dat_s2 = tidy_text %>%
group_by(sent_to) %>%
summarise(count = n(),
length = sum(length, na.rm = T))
#top words by user
tw_s = tidy_words %>%
group_by(sender, words) %>%
summarise(n = n()) %>%
filter(n > 1)
View(tw_s)
ggplot(tw_s, aes(x = sender, y = n)) +
geom_bar()
ggplot(tw_s, aes(x = sender, y = n)) +
geom_bar(stat = "identity")
View(tw_s)
ggplot(tw_s %>% filter(n > 5)
, aes(x = words, y = n, fill = sender, color = sender)) +
geom_bar(stat = "identity")
ggplot(tw_s %>% filter(n > 20)
, aes(x = words, y = n, fill = sender, color = sender)) +
geom_bar(stat = "identity")
ggplot(tw_s %>% filter(n > 10)
, aes(x = words, y = n, fill = sender, color = sender)) +
geom_bar(stat = "identity")
ggplot(tw_s %>% filter(n > 10)
, aes(x = words, y = n, fill = sender, color = sender)) +
geom_bar(stat = "identity")
slice_max
?slice_max
tw_s = tidy_words %>%
group_by(sender, words) %>%
summarise(n = n()) %>%
slice_max(n, 10)
tw_s = tidy_words %>%
group_by(sender, words) %>%
summarise(n = n()) %>%
slice_max(order_by = n,
n = 10)
#top words by user
tw_s = tidy_words %>%
group_by(sender, words) %>%
summarise(n = n()) %>%
slice_max(order_by = n,
n = 10)
View(tw_s)
tw_s = tidy_words %>%
group_by(sender, words) %>%
summarise(n = n())
t = tw_s %>% filter(n > 10
)
View(t)
tw_s = tidy_words %>%
group_by(sender, words) %>%
summarise(n = n()) %>%
ungroup()
t = tw_s %>%
slice_max(order_by = n,
n = 10)
View(t)
t = tw_s %>%
slice_max(order_by = n,
n = 20)
View(t)
View(t)
ggplot(t, aes(x = words, y = n, fill = sender, color = sender)) +
geom_bar(stat = "identity")
ggplot(t, aes(x = n, y = words, fill = sender, color = sender)) +
geom_bar(stat = "identity")
#top words by user
tw_s = tidy_words %>%
group_by(sender, words) %>%
summarise(n = n()) %>%
ungroup()
t = tw_s %>%
slice_max(order_by = n,
n = 25)
#top 25 words
ggplot(t, aes(x = n, y = words, fill = sender, color = sender)) +
geom_bar(stat = "identity")
tidy_text <- dat %>%
mutate(content_c = str_replace_all(content, "[^a-zA-Z0-9]", " "), #remove special characters
content_c = str_to_lower(content_c),                       #make lowercase
content_c = removeWords(content_c, stop_words$word),        #remove stop words
content_c = removeWords(content_c, stop_slang),          #custom words you dont want
content_c = str_squish(content_c),                        #remove whitespace
) %>%
filter(!is.na(content_c),
content_c != "",
content_c != "connected messenger") #removes default message from facebook when you become fb friends with someone
tidy_words = unnest_tokens(tidy_text,
input = content_c,
output = "words",
token = "words")
#SUMMARY ####
dat_s = tidy_text %>%
group_by(sender,sent_to) %>%
summarise(count = n(),
length = sum(length, na.rm = T))
dat_s2 = tidy_text %>%
group_by(sent_to) %>%
summarise(count = n(),
length = sum(length, na.rm = T))
#top words by user
tw_s = tidy_words %>%
group_by(sender, words) %>%
summarise(n = n()) %>%
ungroup()
t = tw_s %>%
slice_max(order_by = n,
n = 25)
#top 25 words
ggplot(t, aes(x = n, y = words, fill = sender, color = sender)) +
geom_bar(stat = "identity")
stop_slang = c("yeah", "yo", "hey")
stop_slang = c("yeah", "yo", "hey")
#CLEANING THE CONTENT
tidy_text <- dat %>%
mutate(content_c = str_replace_all(content, "[^a-zA-Z0-9]", " "), #remove special characters
content_c = str_to_lower(content_c),                       #make lowercase
content_c = removeWords(content_c, stop_words$word),        #remove stop words
content_c = removeWords(content_c, stop_slang),          #custom words you dont want
content_c = str_squish(content_c),                        #remove whitespace
) %>%
filter(!is.na(content_c),
content_c != "",
content_c != "connected messenger") #removes default message from facebook when you become fb friends with someone
tidy_words = unnest_tokens(tidy_text,
input = content_c,
output = "words",
token = "words")
#SUMMARY ####
dat_s = tidy_text %>%
group_by(sender,sent_to) %>%
summarise(count = n(),
length = sum(length, na.rm = T))
dat_s2 = tidy_text %>%
group_by(sent_to) %>%
summarise(count = n(),
length = sum(length, na.rm = T))
#top words by user
tw_s = tidy_words %>%
group_by(sender, words) %>%
summarise(n = n()) %>%
ungroup()
t = tw_s %>%
slice_max(order_by = n,
n = 25)
#top 25 words
ggplot(t, aes(x = n, y = words, fill = sender, color = sender)) +
geom_bar(stat = "identity")
stop_slang = c("yeah", "yo", "hey", "https")
#CLEANING THE CONTENT
tidy_text <- dat %>%
mutate(content_c = str_replace_all(content, "[^a-zA-Z0-9]", " "), #remove special characters
content_c = str_to_lower(content_c),                       #make lowercase
content_c = removeWords(content_c, stop_words$word),        #remove stop words
content_c = removeWords(content_c, stop_slang),          #custom words you dont want
content_c = str_squish(content_c),                        #remove whitespace
) %>%
filter(!is.na(content_c),
content_c != "",
content_c != "connected messenger") #removes default message from facebook when you become fb friends with someone
tidy_words = unnest_tokens(tidy_text,
input = content_c,
output = "words",
token = "words")
#SUMMARY ####
dat_s = tidy_text %>%
group_by(sender,sent_to) %>%
summarise(count = n(),
length = sum(length, na.rm = T))
dat_s2 = tidy_text %>%
group_by(sent_to) %>%
summarise(count = n(),
length = sum(length, na.rm = T))
#top words by user
tw_s = tidy_words %>%
group_by(sender, words) %>%
summarise(n = n()) %>%
ungroup()
t = tw_s %>%
slice_max(order_by = n,
n = 25)
#top 25 words
ggplot(t, aes(x = n, y = words, fill = sender, color = sender)) +
geom_bar(stat = "identity")
stop_slang = c("yeah", "yo", "hey", "haha", "https")
#CLEANING THE CONTENT
tidy_text <- dat %>%
mutate(content_c = str_replace_all(content, "[^a-zA-Z0-9]", " "), #remove special characters
content_c = str_to_lower(content_c),                       #make lowercase
content_c = removeWords(content_c, stop_words$word),        #remove stop words
content_c = removeWords(content_c, stop_slang),          #custom words you dont want
content_c = str_squish(content_c),                        #remove whitespace
) %>%
filter(!is.na(content_c),
content_c != "",
content_c != "connected messenger") #removes default message from facebook when you become fb friends with someone
tidy_words = unnest_tokens(tidy_text,
input = content_c,
output = "words",
token = "words")
#SUMMARY ####
dat_s = tidy_text %>%
group_by(sender,sent_to) %>%
summarise(count = n(),
length = sum(length, na.rm = T))
dat_s2 = tidy_text %>%
group_by(sent_to) %>%
summarise(count = n(),
length = sum(length, na.rm = T))
#top words by user
tw_s = tidy_words %>%
group_by(sender, words) %>%
summarise(n = n()) %>%
ungroup()
t = tw_s %>%
slice_max(order_by = n,
n = 25)
#top 25 words
ggplot(t, aes(x = n, y = words, fill = sender, color = sender)) +
geom_bar(stat = "identity")
stop_slang = c("yeah", "yo", "hey", "haha", "lot ","https")
#CLEANING THE CONTENT
tidy_text <- dat %>%
mutate(content_c = str_replace_all(content, "[^a-zA-Z0-9]", " "), #remove special characters
content_c = str_to_lower(content_c),                       #make lowercase
content_c = removeWords(content_c, stop_words$word),        #remove stop words
content_c = removeWords(content_c, stop_slang),          #custom words you dont want
content_c = str_squish(content_c),                        #remove whitespace
) %>%
filter(!is.na(content_c),
content_c != "",
content_c != "connected messenger") #removes default message from facebook when you become fb friends with someone
tidy_words = unnest_tokens(tidy_text,
input = content_c,
output = "words",
token = "words")
#SUMMARY ####
dat_s = tidy_text %>%
group_by(sender,sent_to) %>%
summarise(count = n(),
length = sum(length, na.rm = T))
dat_s2 = tidy_text %>%
group_by(sent_to) %>%
summarise(count = n(),
length = sum(length, na.rm = T))
#top words by user
tw_s = tidy_words %>%
group_by(sender, words) %>%
summarise(n = n()) %>%
ungroup()
t = tw_s %>%
slice_max(order_by = n,
n = 25)
#top 25 words
ggplot(t, aes(x = n, y = words, fill = sender, color = sender)) +
geom_bar(stat = "identity")
stop_slang = c("yeah", "yo", "hey", "haha", "a lot","https")
#CLEANING THE CONTENT
tidy_text <- dat %>%
mutate(content_c = str_replace_all(content, "[^a-zA-Z0-9]", " "), #remove special characters
content_c = str_to_lower(content_c),                       #make lowercase
content_c = removeWords(content_c, stop_words$word),        #remove stop words
content_c = removeWords(content_c, stop_slang),          #custom words you dont want
content_c = str_squish(content_c),                        #remove whitespace
) %>%
filter(!is.na(content_c),
content_c != "",
content_c != "connected messenger") #removes default message from facebook when you become fb friends with someone
tidy_words = unnest_tokens(tidy_text,
input = content_c,
output = "words",
token = "words")
#SUMMARY ####
dat_s = tidy_text %>%
group_by(sender,sent_to) %>%
summarise(count = n(),
length = sum(length, na.rm = T))
dat_s2 = tidy_text %>%
group_by(sent_to) %>%
summarise(count = n(),
length = sum(length, na.rm = T))
#top words by user
tw_s = tidy_words %>%
group_by(sender, words) %>%
summarise(n = n()) %>%
ungroup()
t = tw_s %>%
slice_max(order_by = n,
n = 25)
#top 25 words
ggplot(t, aes(x = n, y = words, fill = sender, color = sender)) +
geom_bar(stat = "identity")
View(tidy_words)
tidy_text <- dat %>%
mutate(content_c = str_replace_all(content, "[^a-zA-Z0-9]", " "), #remove special characters
content_c = str_to_lower(content_c),                       #make lowercase
content_c = removeWords(content_c, stop_words$word),        #remove stop words
content_c = removeWords(content_c, stop_slang),          #custom words you dont want
content_c = str_squish(content_c),                        #remove whitespace
content_cc = content_c #create copy of column for the unnest token
) %>%
filter(!is.na(content_c),
content_c != "",
content_c != "connected messenger") #removes default message from facebook when you become fb friends with someone
#may want to filter by dictionary words as well TBD
tidy_words = unnest_tokens(tidy_text,
input = content_cc,
output = "words",
token = "words")
View(tidy_text)
View(tidy_words)
tidy_text <- dat %>%
mutate(content_c = str_to_lower(content_c),                       #make lowercase
content_c = removeWords(content_c, stop_words$word),        #remove stop words
content_c = str_replace_all(content, "[^a-zA-Z0-9]", " "), #remove special characters
content_c = removeWords(content_c, stop_slang),          #custom words you dont want
content_c = str_squish(content_c),                        #remove whitespace
content_cc = content_c #create copy of column for the unnest token
) %>%
filter(!is.na(content_c),
content_c != "",
content_c != "connected messenger") #removes default message from facebook when you become fb friends with someone
#may want to filter by dictionary words as well TBD
tidy_words = unnest_tokens(tidy_text,
input = content_cc,
output = "words",
token = "words")
#SUMMARY ####
dat_s = tidy_text %>%
group_by(sender,sent_to) %>%
summarise(count = n(),
length = sum(length, na.rm = T))
dat_s2 = tidy_text %>%
group_by(sent_to) %>%
summarise(count = n(),
length = sum(length, na.rm = T))
#top words by user
tw_s = tidy_words %>%
group_by(sender, words) %>%
summarise(n = n()) %>%
ungroup()
t = tw_s %>%
slice_max(order_by = n,
n = 25)
#top 25 words
ggplot(t, aes(x = n, y = words, fill = sender, color = sender)) +
geom_bar(stat = "identity")
tidy_text <- dat %>%
mutate(content_c = str_replace_all(content, "[^a-zA-Z0-9]", " "), #remove special characters
content_c = str_to_lower(content_c),                       #make lowercase
content_c = removeWords(content_c, stop_words$word),        #remove stop words
content_c = removeWords(content_c, stop_slang),          #custom words you dont want
content_c = str_squish(content_c),                        #remove whitespace
content_cc = content_c #create copy of column for the unnest token
) %>%
filter(!is.na(content_c),
content_c != "",
content_c != "connected messenger") #removes default message from facebook when you become fb friends with someone
#may want to filter by dictionary words as well TBD
tidy_words = unnest_tokens(tidy_text,
input = content_cc,
output = "words",
token = "words")
#SUMMARY ####
dat_s = tidy_text %>%
group_by(sender,sent_to) %>%
summarise(count = n(),
length = sum(length, na.rm = T))
dat_s2 = tidy_text %>%
group_by(sent_to) %>%
summarise(count = n(),
length = sum(length, na.rm = T))
#top words by user
tw_s = tidy_words %>%
group_by(sender, words) %>%
summarise(n = n()) %>%
ungroup()
t = tw_s %>%
slice_max(order_by = n,
n = 25)
#top 25 words
ggplot(t, aes(x = n, y = words, fill = sender, color = sender)) +
geom_bar(stat = "identity")
View(tidy_words)
