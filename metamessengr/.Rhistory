### Script to analyze messenger data ###
using<-function(...) {
libs<-unlist(list(...))
req<-unlist(lapply(libs,require,character.only=TRUE))
need<-libs[req==FALSE]
if(length(need)>0){
install.packages(need)
lapply(need,require,character.only=TRUE)
}
}
using("purrr","jsonlite","dplyr","tidytext","tidyr","textdata","stringr",
"ggplot2","scales","chron","lexicon","ggraph","igraph")
#TOBY IS GETTING FORKED!
### Script to analyze messenger data ###
using<-function(...) {
libs<-unlist(list(...))
req<-unlist(lapply(libs,require,character.only=TRUE))
need<-libs[req==FALSE]
if(length(need)>0){
install.packages(need)
lapply(need,require,character.only=TRUE)
}
}
using("purrr","jsonlite","dplyr","tidytext","tidyr","textdata","stringr",
"ggplot2","scales","chron","lexicon","ggraph","igraph")
## Writing a function for just selection
selection <- function(inp=NULL) {
## This looks for all of the files that start with "message_" and are .json
## format. It then reads each file in using fromJSON() and assigns to inp
inp = lapply(list.files(recursive = T, pattern = "\\.json$"), fromJSON)
## This then takes the inp and finds how many files it read and assigns it l
l = as.numeric(length(map(inp,2)))
## s is called as a sequence from 1 to the previous length (l)
s = seq(from = 1, to = l)
## Setting f to NULL
f = NULL
## For i in the number of files loaded, select the timestamp, sender_name, and
## content of the message. It then makes f = to that selection if it's the
## first file's data, or it row binds the data onto it if it's anything else.
for (i in s) {
if("content" %in% names(inp[[i]][[2]])){
t = select(map(inp,2)[[i]], timestamp_ms, sender_name, content)
if (i == 1){
f=t
} else {
f = rbind(f, t)
}
}
print(i)
}
## Then f, the smaller dataset, is bound to inp and is the output
inp = f
}
n = selection()
## Convert n to a data frame
n = as.data.frame(n)
## Flattenn the nested dataframe
n = flatten(n)
#TOBY IS GETTING FORKED!
### Script to analyze messenger data ###
using<-function(...) {
libs<-unlist(list(...))
req<-unlist(lapply(libs,require,character.only=TRUE))
need<-libs[req==FALSE]
if(length(need)>0){
install.packages(need)
lapply(need,require,character.only=TRUE)
}
}
using("purrr","jsonlite","dplyr","tidytext","tidyr","textdata","stringr",
"ggplot2","scales","chron","lexicon","ggraph","igraph")
selection <- function(inp=NULL) {
## This looks for all of the files that start with "message_" and are .json
## format. It then reads each file in using fromJSON() and assigns to inp
inp = lapply(list.files(recursive = T, pattern = "\\.json$"), fromJSON)
## This then takes the inp and finds how many files it read and assigns it l
l = as.numeric(length(map(inp,2)))
## s is called as a sequence from 1 to the previous length (l)
s = seq(from = 1, to = l)
## Setting f to NULL
f = NULL
## For i in the number of files loaded, select the timestamp, sender_name, and
## content of the message. It then makes f = to that selection if it's the
## first file's data, or it row binds the data onto it if it's anything else.
for (i in s) {
if("content" %in% names(inp[[i]][[2]])){
t = select(map(inp,2)[[i]], timestamp_ms, sender_name, content)
if (i == 1){
f=t
} else {
f = rbind(f, t)
}
}
print(i)
}
## Then f, the smaller dataset, is bound to inp and is the output
inp = f
}
n = selection()
setwd("messenger_data/messages/inbox/")
## Writing a function for just selection
selection <- function(inp=NULL) {
## This looks for all of the files that start with "message_" and are .json
## format. It then reads each file in using fromJSON() and assigns to inp
inp = lapply(list.files(recursive = T, pattern = "\\.json$"), fromJSON)
## This then takes the inp and finds how many files it read and assigns it l
l = as.numeric(length(map(inp,2)))
## s is called as a sequence from 1 to the previous length (l)
s = seq(from = 1, to = l)
## Setting f to NULL
f = NULL
## For i in the number of files loaded, select the timestamp, sender_name, and
## content of the message. It then makes f = to that selection if it's the
## first file's data, or it row binds the data onto it if it's anything else.
for (i in s) {
if("content" %in% names(inp[[i]][[2]])){
t = select(map(inp,2)[[i]], timestamp_ms, sender_name, content)
if (i == 1){
f=t
} else {
f = rbind(f, t)
}
}
print(i)
}
## Then f, the smaller dataset, is bound to inp and is the output
inp = f
}
n = selection()
View(n)
n = as.data.frame(n)
## Flattenn the nested dataframe
n = flatten(n)
## Turn the timestamp into the format that R likes (not miliseconds)
n$timestamp_ms = n$timestamp_ms/1000
## Have it convert the timestamp to POSIXct
n$timestamp_ms <- as.POSIXct(n$timestamp_ms, origin="1970-01-01", tz="America/Los_Angeles")
## Make a new row, "date" and have it read in the POSIXct as date
n$date = as.Date(as.character(n$timestamp_ms))
## Read in the timestamp time as an actual time (this separates the date and time)
n$time = strftime(n$timestamp_ms, format="%H:%M:%S")
## Further time cleaning
n$time = chron(times=n$time)
n$time = as.numeric(n$time)
## New column with sending hour
n$hour = n$time*24
## New column with the sender's name
n$sender = n$sender_name
## new column with the length of each message sent
n$length=nchar(n$content)
## Messenger only allows messages of 640 characters or less. Setting anything
## above that to NA as anything else is bogus
n$length[which(n$length>640)] = NA
View(n)
toby = n %>%
group_by(sender) %>%
summarise(count = n())
View(toby)
View(toby)
toby = n %>%
group_by(sender) %>%
summarise(count = n()) %>%
filter(count > 100)
View(toby)
toby = n %>%
group_by(sender) %>%
summarise(count = n())
data("grady_augmented")
### Turning it into a tibble that works
grady_augmented=as_tibble(grady_augmented)
grady_augmented=rename(grady_augmented, word=value)
## Removing some custom stopwords
custom_stop_words <- bind_rows(tibble(word = c("na"),
lexicon = c("custom")),
stop_words)
View(custom_stop_words)
tidy_text <- n %>%
unnest_tokens(word, text)
View(grady_augmented)
tidy_text <- n %>%
mutate(content_clean = str_extract(content %in% grady_augmented))
### Adding whole dictionary
data("grady_augmented")
### All words in one column then removing stop words and only keeping words in the dictionary
tidy_text <- n %>%
mutate(content_clean = str_extract(content %in% grady_augmented))
View(n)
### Graphing the words
tidy_text %>%
count(word, sort = TRUE) %>%
filter(n > 1500) %>%
mutate(word = reorder(word, n)) %>%
ggplot(aes(word, n)) +
geom_col() +
xlab(NULL) +
coord_flip()
getwd()
