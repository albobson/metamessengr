group_sum(me_clean)
top_words(me_clean)
top_words(me_clean) %>%
filter(sender = "Alexander*")
top_words(me_clean)
top_words(me_clean) %>%
filter(sender == "Alexander James Robertson")
top_words(me_clean) %>%
filter("sender" == "Alexander James Robertson")
me_cut <- mess_cut_off(me_clean)
me_cut
unique(me_cut$event)
me_clean %>%
dplyr::mutate(event = base::ifelse(date < base::as.Date(time, origin = "1970-01-01"),
"pre", #if before cutoff time
"post")) #if before cutoff time
test <- me_clean %>%
dplyr::mutate(event = base::ifelse(date < base::as.Date(time, origin = "1970-01-01"),
"pre", #if before cutoff time
"post")) #if before cutoff time
unique(test$event)
mess_cut_off <-  function(data, time = "2020-03-01") {
data %>%
dplyr::mutate(event = dplyr::ifelse(date < base::as.Date(time, origin = "1970-01-01"),
"pre", #if before cutoff time
"post")) #if before cutoff time
}
test <- me_clean %>%
dplyr::mutate(event = base::ifelse(date < base::as.Date(time, origin = "1970-01-01"),
"pre", #if before cutoff time
"post")) #if before cutoff time
me_cut <- mess_cut_off(me_clean)
mess_cut_off <-  function(data, time = "2020-03-01") {
data %>%
dplyr::mutate(event = dplyr::if_else(date < base::as.Date(time, origin = "1970-01-01"),
"pre", #if before cutoff time
"post")) #if before cutoff time
}
me_cut <- mess_cut_off(me_clean)
me_cut
unique(test$event)
test <- me_clean %>%
dplyr::mutate(event = base::ifelse(date < base::as.Date("2020-03-01", origin = "1970-01-01"),
"pre", #if before cutoff time
"post")) #if before cutoff time
unique(test$event)
time = "2020-03-01"
test <- me_clean %>%
dplyr::mutate(event = base::ifelse(date < base::as.Date(time),
"pre", #if before cutoff time
"post"))
test <- me_clean %>%
dplyr::mutate(event = base::ifelse(date < base::as.Date(time, origin = "1970-01-01"),
"pre", #if before cutoff time
"post"))
unique(test$event)
mess_cut_off <-  function(data, time = "2020-03-01") {
data %>%
dplyr::mutate(event = base::ifelse(time < base::as.Date(time, origin = "1970-01-01"),
"pre", #if before cutoff time
"post")) #if before cutoff time
}
test <- mess_cut_off(me_clean)
unique(test$event)
me_clean
mess_cut_off <-  function(data, time = "2020-03-01") {
data %>%
dplyr::mutate(event = base::ifelse(date < base::as.Date(time, origin = "1970-01-01"),
"pre", #if before cutoff time
"post")) #if before cutoff time
}
test <- mess_cut_off(me_clean, time = "2020-03-01")
unique(test$event)
mess_cut_off <-  function(data, time = "2020-03-01") {
co <- base::as.Date(time, origin = "1970-01-01")
data %>%
dplyr::mutate(event = base::ifelse(date < co,
"pre", #if before cutoff time
"post")) #if before cutoff time
}
test <- mess_cut_off(me_clean, time = "2020-03-01")
unique(test$event)
test$event
summary(test$event)
table(test$event)
t_mess(test)
plot_mess(test)
plot_mess <- function(data) {
timestamp_ms <- value <- event <- vars <- sender <- NULL
event_ts_p =
ggplot2::ggplot(data,
ggplot2::aes(x= timestamp_ms, y = value, fill = event, color = event )) +
ggplot2::geom_col(show.legend = F) +
ggplot2::facet_wrap(ggplot2::vars(sender))
sentiment_hist =
ggplot2::ggplot(data,
ggplot2::aes(value, fill= event, color = event)) +
ggplot2::geom_density(position = "dodge", na.rm = T, alpha = .5) +
ggplot2::facet_wrap(ggplot2::vars(sender))
event_ts_p
sentiment_hist
}
plot_mess(test)
ggplot2::ggplot(test,
ggplot2::aes(x= timestamp_ms, y = value, fill = event, color = event )) +
ggplot2::geom_col(show.legend = F) +
ggplot2::facet_wrap(ggplot2::vars(sender))
test
me_cut <- mess_cut_off(me_clean)
me_cut
unique(me_cut$event)
me_sent <- mess_sentiment(me_cut)
t_mess(me_sent)
t_mess <-  function(data){
event <- value <- NULL
x = data %>%
dplyr::filter(event == "pre") %>%
dplyr::select(value)
y = data %>%
dplyr::filter(event == "post") %>%
dplyr::select(value)
t_test = stats::t.test(x,y)
t_test
}
t_mess(me_sent)
plot_mess(me_sent)
ggplot2::ggplot(me_sent,
ggplot2::aes(x= timestamp_ms, y = value, fill = event, color = event )) +
ggplot2::geom_col(show.legend = F) +
ggplot2::facet_wrap(ggplot2::vars(sender))
alex <- me_sent %>%
filter(sender="Alexander James Robertson")
alex <- me_sent %>%
filter(sender == "Alexander James Robertson")
names(me_sent)
alex <- me_sent[which(me_sent$sender == "Alexander James Robertson")]
alex <- me_sent[,which(me_sent$sender == "Alexander James Robertson")]
alex <- me_sent[which(me_sent$sender == "Alexander James Robertson"),
]
alex
alex$content_unclean[55555]
alex$content_unclean[5555]
alex$content_unclean[5556]
alex$content_unclean[553]
alex$content_unclean[53]
h <- ggplot(alex, aes(x=date, y=hour, color=sender))
h + geom_point(size=1) +
scale_y_continuous(limits = c(0, 24), breaks = (0:12)*2)
h <- ggplot(alex, aes(x=date, y=hour, color=sender))
h + geom_point(size=1) +
scale_y_continuous(limits = c(0, 24), breaks = (0:12)*2)
## Scatter of messages sent by time
h <- ggplot(alex, aes(x=date, y=hour, color=sender))
plot_mess(alex)
ggplot2::ggplot(alex,
ggplot2::aes(x= timestamp_ms, y = value, fill = event, color = event )) +
ggplot2::geom_col(show.legend = F) +
ggplot2::facet_wrap(ggplot2::vars(sender))
min(alex$value)
min(alex$value,na.rm = TRUE)
library(plotly)
ggplotly(ggplot2::ggplot(alex,
ggplot2::aes(x= timestamp_ms, y = value, fill = event, color = event )) +
ggplot2::geom_col(show.legend = F) +
ggplot2::facet_wrap(ggplot2::vars(sender)))
plot_mess <- function(data) {
timestamp_ms <- value <- event <- vars <- sender <- NULL
event_ts_p =
ggplot2::ggplot(data,
ggplot2::aes(x= timestamp_ms, y = value, fill = event, color = event )) +
ggplot2::geom_col(show.legend = F) +
ggplot2::facet_wrap(ggplot2::vars(sender))
sentiment_hist =
ggplot2::ggplot(data,
ggplot2::aes(value, fill= event, color = event)) +
ggplot2::geom_density(position = "dodge", na.rm = T, alpha = .5) +
ggplot2::facet_wrap(ggplot2::vars(sender))
sentiment_hist; event_ts_p
}
plot_mess(me_sent)
document()
rm(list = c("group_sum", "mess_cut_off", "t_mess"))
document()
check()
check()
library(devtools)
install()
library(metamessengr)
mess_selection("C:/Users/meowy/OneDrive/Desktop/fbm_data_subset")
me <- mess_selection("C:/Users/meowy/OneDrive/Desktop/fbm_data_subset")
me <- clean_mess_all(me)
sender_sum(me)
sender_sum <- function(data) {
sender <- sent_to <- count <- NULL
data %>%
dplyr::group_by(sender,sent_to) %>%
dplyr::summarise(count = dplyr::n(),
length = base::sum(length, na.rm = T)) %>%
dplyr::arrange(dplyr::desc(count))
}
sender_sum(me)
group_sum(me)
me_sent <- mess_sentiment(me)
me_sent <- mess_cut_off(me_sent)
me_sent
unique(me_sent$event)
t_mess(me_sent)
t_mess(me_sent[which(me$convo=="Olivia Wang")])
plot_senti_time(me_sent)
plot_senti_time(me_sent, wrap_by = sender)
plot_senti_time <- function(data) {
timestamp_ms <- value <- event <- vars <- sender <- NULL
event_ts_p =
ggplot2::ggplot(data,
ggplot2::aes(x= timestamp_ms, y = value, fill = event, color = event )) +
ggplot2::geom_col(show.legend = F) +
ggplot2::facet_wrap(ggplot2::vars(sender))
event_ts_p
}
plot_senti_time(me_sent)
\
library(usethis)
library(devtools)
use_vignette()
?use_vignette
use_vignette(name="run_through", title = "General Set-Up and Usage")
library(metamessengr)
file_loc = "C:/Users/meowy/OneDrive/Desktop/fbm_data_subset"
## Enter your file location
me <- mess_selection(file_loc = file_loc)
## Enter your file location
me <- mess_selection(file_loc = file_loc)
me
names(me)
me_clean <- mess_clean_all(me)
library(metamessengr)
me_clean <- mess_clean_all(me)
me_clean <- metamessengr::mess_clean_all(me)
me_clean <- clean_mess_all(me)
top_words <- function(data, num) {
total <- freq <- value <- content <- sender <- words <- used <- NULL
grady_augmented <- dplyr::as_tibble(metamessengr::grady_augmented)
grady_augmented=dplyr::rename(grady_augmented, words=value)
data = data %>%
tidytext::unnest_tokens(input = content,
output = "words",
token = "words") %>%
dplyr::semi_join(grady_augmented) %>%
dplyr::group_by(sender, words) %>%
dplyr::summarise(used = dplyr::n()) %>%
dplyr::mutate(total = base::sum(used)) %>%
dplyr::mutate(freq = used/total) %>%
dplyr::ungroup() %>%
dplyr::mutate(sender = base::ifelse(sender=="",NA,sender)) %>%
stats::na.omit()
df = data %>%
dplyr::group_by(sender) %>%
dplyr::arrange(dplyr::desc(freq), .by_group = TRUE)
df = data %>%
dplyr::slice(1:num)
df
}
top_words(me_clean, num=3)
me_clean
top_words(me_clean)
top_words <- function(data) {
total <- freq <- value <- content <- sender <- words <- used <- NULL
grady_augmented <- dplyr::as_tibble(metamessengr::grady_augmented)
grady_augmented=dplyr::rename(grady_augmented, words=value)
data = data %>%
tidytext::unnest_tokens(input = content,
output = "words",
token = "words") %>%
dplyr::semi_join(grady_augmented) %>%
dplyr::group_by(sender, words) %>%
dplyr::summarise(used = dplyr::n()) %>%
dplyr::mutate(total = base::sum(used)) %>%
dplyr::mutate(freq = used/total) %>%
dplyr::ungroup() %>%
dplyr::mutate(sender = base::ifelse(sender=="",NA,sender)) %>%
stats::na.omit()
df = data %>%
dplyr::group_by(sender) %>%
dplyr::arrange(dplyr::desc(freq), .by_group = TRUE)
# df = data %>%
#   dplyr::slice(1:num)
df
}
top_words(me_clean)
top <- top_words(me_clean)
unique(top$sender)
top_words <- function(data, num) {
total <- freq <- value <- content <- sender <- words <- used <- NULL
grady_augmented <- dplyr::as_tibble(metamessengr::grady_augmented)
grady_augmented=dplyr::rename(grady_augmented, words=value)
data = data %>%
tidytext::unnest_tokens(input = content,
output = "words",
token = "words") %>%
dplyr::semi_join(grady_augmented) %>%
dplyr::group_by(sender, words) %>%
dplyr::summarise(used = dplyr::n()) %>%
dplyr::mutate(total = base::sum(used)) %>%
dplyr::mutate(freq = used/total) %>%
dplyr::ungroup() %>%
dplyr::mutate(sender = base::ifelse(sender=="",NA,sender)) %>%
stats::na.omit()
df = data %>%
dplyr::group_by(sender) %>%
dplyr::arrange(dplyr::desc(freq), .by_group = TRUE)
df = data %>%
dplyr::group_by(sender) %>%
dplyr::slice(1:num)
df
}
top_words(me_clean, num=3)
top_words <- function(data, num) {
total <- freq <- value <- content <- sender <- words <- used <- NULL
grady_augmented <- dplyr::as_tibble(metamessengr::grady_augmented)
grady_augmented=dplyr::rename(grady_augmented, words=value)
data = data %>%
tidytext::unnest_tokens(input = content,
output = "words",
token = "words") %>%
dplyr::semi_join(grady_augmented) %>%
dplyr::group_by(sender, words) %>%
dplyr::summarise(used = dplyr::n()) %>%
dplyr::mutate(total = base::sum(used)) %>%
dplyr::mutate(freq = used/total) %>%
dplyr::ungroup() %>%
dplyr::mutate(sender = base::ifelse(sender=="",NA,sender)) %>%
stats::na.omit()
df = data %>%
dplyr::group_by(sender) %>%
dplyr::arrange(dplyr::desc(freq), .by_group = TRUE)
df = data %>%
dplyr::group_by(sender, freq) %>%
dplyr::slice(1:num)
df
}
top_words(me_clean, num=3)
top_words <- function(data, num) {
total <- freq <- value <- content <- sender <- words <- used <- NULL
grady_augmented <- dplyr::as_tibble(metamessengr::grady_augmented)
grady_augmented=dplyr::rename(grady_augmented, words=value)
data = data %>%
tidytext::unnest_tokens(input = content,
output = "words",
token = "words") %>%
dplyr::semi_join(grady_augmented) %>%
dplyr::group_by(sender, words) %>%
dplyr::summarise(used = dplyr::n()) %>%
dplyr::mutate(total = base::sum(used)) %>%
dplyr::mutate(freq = used/total) %>%
dplyr::ungroup() %>%
dplyr::mutate(sender = base::ifelse(sender=="",NA,sender)) %>%
stats::na.omit()
df = data %>%
dplyr::group_by(sender) %>%
dplyr::arrange(dplyr::desc(freq), .by_group = TRUE)
df = data %>%
dplyr::group_by(sender) %>%
dplyr::arrange(dplyr::desc(freq), .by_group = TRUE) %>%
dplyr::slice(1:num)
df
}
top_words(me_clean, num=3)
top_words(me_clean, num=200)
document()
rm(list = c("top_words"))
document()
document()
document()
check()
check()
check()
build()
build()
library(tidymodels)
library(tidyverse)
sentiment_ts <- mess_sentiment(me_clean)
#******************************************************************************************
#test for normalcy
#******************************************************************************************
ggplot(sentiment_ts, aes(value, fill = event))+
geom_histogram()
sentiment_ts <- mess_cut_off(me_clean, time = "2020-03-01")
sentiment_ts <- mess_sentiment(sentiment_ts)
#******************************************************************************************
#test for normalcy
#******************************************************************************************
ggplot(sentiment_ts, aes(value, fill = event))+
geom_histogram()
#t test function
x = sentiment_ts %>%
filter(event == "pre") %>%
select(value)
y = sentiment_ts %>%
filter(event == "post") %>%
select(value)
t_test = t.test(x,y)
t_test
x = sentiment_ts %>%
filter(sender == "Alexander James Robertson",
event == "pre") %>%
select(value)
y = sentiment_ts %>%
filter(sender == "Alexander James Robertson",
event == "post") %>%
select(value)
personal_t_test = t.test(x,y)
personal_t_test
#******************************************************************************************
#ANOVA WITH SENDER
#******************************************************************************************
two.way <- aov(value ~ event * sender, data = sentiment_ts)
summary(two.way)
p = ggplot(sentiment_ts %>% filter(value != 0), #with zero it looks weird not sure if this is appropriate
aes(value, fill = event)) +
geom_boxplot() +
facet_wrap(vars(sender)) +
xlab("Sentiment") +
ggtitle("BOXPLOT OF SENTIMENT BY SENDER")
png("boxplot.png")
p
dev.off()
#CRUDE
crude_mod_fun = function(data) {
mod = lm(value ~ event,
data = data)
tidy(mod) %>%
mutate(ucl = estimate + std.error,
lcl = estimate - std.error,
term = if_else(term == "eventpost",
"crude",
term))
}
crude_mod = crude_mod_fun(sentiment_ts)
summary(crude_mod)
crude_mod
#ADJUSTED
adj_mod_fun = function(data) {
mod = lm(value ~ event * sender,
data = data)
tidy(mod) %>%
mutate(ucl = estimate + std.error,
lcl = estimate - std.error,
term = if_else(term == "eventpost",
"adjusted",
term)
)
}
adj_mod = adj_mod_fun(sentiment_ts)
adj_mod
#COMBINED FOR GGPLOT
mod_df = rbind.data.frame(crude_mod,
adj_mod
) %>%
mutate_if(is.numeric, round, digits = 2) %>%
filter(term != "(Intercept)",
!grepl("^sender",term))
p2 = ggplot(mod_df,
aes(x = term, y = estimate, ymin = lcl, ymax = ucl)) +
geom_pointrange(aes(col = term), position=position_dodge(width=0.30)) +
ylab("Sentiment RR & 95% CI") +
geom_hline(aes(yintercept = 0)) +
# scale_color_brewer(palette= "Dark2", name = "Treatment Group") +
xlab("Sender") +
theme(axis.text.x = element_text(angle = 45, vjust = 1, hjust=1, color = "black", size = 12)) +
ggtitle("Estimates of Sentiment Post Event compared to User")
png("adjusted RR estimates.png")
p2
dev.off()
p2 = ggplot(mod_df,
aes(x = term, y = estimate, ymin = lcl, ymax = ucl)) +
geom_pointrange(aes(col = term), position=position_dodge(width=0.30)) +
ylab("Sentiment RR & 95% CI") +
geom_hline(aes(yintercept = 0)) +
# scale_color_brewer(palette= "Dark2", name = "Treatment Group") +
xlab("Sender") +
theme(axis.text.x = element_text(angle = 45, vjust = 1, hjust=1, color = "black", size = 12)) +
ggtitle("Estimates of Sentiment Post Event compared to User") +
labs(legend = "")
png("adjusted RR estimates.png")
png("adjusted RR estimates.png")
p2
dev.off()
p2
dev.off()
sender_sum(clean_me)
sender_sum(me_clean)
group_sum(me_clean)
?sum
?clean_mess_all
View(p2)
me_s <- mess_sentiment(me_clean)
plot_mess_dens(me_s)
me <- mess_sentiment("C:/Users/meowy/OneDrive/Desktop/fbm_data_subset")
me <- mess_selection("C:/Users/meowy/OneDrive/Desktop/fbm_data_subset")
me_c <- clean_mess_all(me)
me_co <- mess_cut_off(me_c)
me_c <- mess_sentiment(me_co)
plot_mess_dens(me_c)
plot_senti_time(me_c)
document()
install()
library(metamessengr)
check()
library(devtools)
check()
build()
build()
