---
title: "General Set-Up and Usage"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{General Set-Up and Usage}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)

file_loc = "C:/Users/meowy/OneDrive/Desktop/fbm_data_subset"
```

#### This vignette will walk you through the general set-up and usage of the metamessengr package. 

```{r setup}
library(metamessengr)
```

##### Selecting and Reading in Data

The first and most important function of the package is the `mess_sentiment()` function. This function allows one to parse through Facebook's JSON formatted Messenger data and select the most important information - who sent what, what did they send, and when did they send it.

The function is recursive, so all that needs to be specified is the folder that contains all of the downloaded and unzipped data from Facebook. The conversations will be in a file called `"/inbox`. The output will be a data frame with 4 variables: the timestamp, the sender, the content, and which chat it was sent to.

```{r}
## Enter your file location
me <- mess_selection(file_loc = file_loc)

me
```

##### Cleaning Selected Data

The newly imported data needs to be cleaned. This can be done piece-wise, through the use of the `clean_mess_text()` and `clean_mess_time()` functions, or it can be done all at once using `clean_mess_all()`. For most situations, it's best to use `clean_mess_all()` to reduce the potential for user error.

Stop words ("as", "the", "a", etc.) are generally removed before text analysis, as they do not contribute meaningfully to context. These are removed during the cleaning process. If there are additional words that you would like to remove, use the `custom_clean` variable, which is by default set to `NULL`. 

The timestamp is by default set to Los Angeles/Pacific time. To set a different timezone, set it using `timezone=`.

```{r}
me <- clean_mess_all(me)
```

##### Summarizing the Data

Now we have a cleaned, usable data set for data analysis. The first thing that we can do is run a few general summaries of the data. 

First, let's take a look at the senders. How many messages did each person send, and what was the total number of characters sent?

```{r}
sender_sum(me)
```

We can also look at the summary of each group. How many messages were sent to the group and what where the total number of characters that were sent?

```{r}
group_sum(me)
```

Finally, we can also analyze the top words that each person sent, and look at the frequency of those words. The `num=` argument in the function specifies how many top messages you want per person.

```{r}
top_words(me, num=5)
```

##### Sentiment Analysis

The words that we use have implicit sentiments associated with them. For example, "death" has a negative association, while "money" may have a positive association. There have been many studies looking at the average association of each word. In this Vignette, we will be looking at the sentiment as described by Finn Ã…rup Nielsen. Nielsen associated each word in the English language with a score between -5 and +5. 

We were interested in looking at if there was a difference in overall sentiment used before and after an event. We expected that there would be a dip in the overall sentiment of each message after the beginning of the COVID-19 pandemic. In this code, we assume the pandemic "started" in the USA on 2020-03-01, since that was around the time that lock-down restrictions took place.

Using the `cut_off()` function, we can add a column which denotes if a message was "pre" or "post" an event time.

```{r}
me <- mess_cut_off(me, time = "2020-03-01")
```

We can break down each message into one word chunks and associate each word's corresponding sentiment using the `mess_sentiment()` function.

```{r}
me <- mess_sentiment(me)
```

Now that we have our associated word sentiments and whether they were sent before our specified event, we can run statistical tests on the data and begin visualizations.

First, we can run a t-test on the entire data set to see if there is a difference across all messages 

```{r}
t_mess(me)
```

In this case, we can see that there is a significantly lower sentiment after the event.

We can further visualize this by plotting the sentiment over time. The 
`plot_senti_time()` function will plot the sentiment of each message over time for each sender. Currently, it will facet wrap all senders. Future development of the code will allow more control over which senders to plot.

```{r}
plot_senti_time(me)
```

We can also plot the density of the sentiment scores before and after the event using `plot_mess_dens()`. The default for this plot is to facet wrap by sender. In the future, we plan to add an arguement which would allow one to change the facet wrap variable.

```{r}
plot_mess_dens(me)
```
